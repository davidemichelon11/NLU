{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoDcrDK8PsAqTRTjKGBrIU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidemichelon11/NLU/blob/main/NLU_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfoDANvvOpeO"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('punkt')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "mr = movie_reviews\n",
        "neg = mr.paras(categories='neg')\n",
        "pos = mr.paras(categories='pos')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BASELINE via SVM**"
      ],
      "metadata": {
        "id": "IhxZwaOvaBcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "VK4cuQS0aF4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Considering the constant TRAINING_SPLIT, the function spits between training and test (data and labels)\n",
        "def split_samples(cor, val, train_samples, test_samples, train_labels, test_labels):\n",
        "  training_limit = int(len(cor)*TRAINING_SPLIT)\n",
        "  for i, value in enumerate(cor):\n",
        "    if i < training_limit:\n",
        "      train_samples.append(value)\n",
        "      train_labels.append(val)\n",
        "    else:\n",
        "      test_samples.append(value)\n",
        "      test_labels.append(val)"
      ],
      "metadata": {
        "id": "Kw-u9youOCN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "import random\n",
        "def compute_SVM(pos, neg):\n",
        "  vectorizer = CountVectorizer()\n",
        "  classifier2 = SVC()\n",
        "  TRAINING_SPLIT = 0.80\n",
        "    \n",
        "  train_samples = []\n",
        "  train_labels = []\n",
        "  test_samples = []\n",
        "  test_labels = []\n",
        "\n",
        "  split_samples([str(d) for d in neg], 0, train_samples, test_samples, train_labels, test_labels)\n",
        "  split_samples([str(d) for d in pos], 1, train_samples, test_samples, train_labels, test_labels)\n",
        "\n",
        "  vectors = vectorizer.fit_transform(train_samples + test_samples)\n",
        "  classifier2.fit(vectors[:len(train_samples)], train_labels)\n",
        "  labels_pred = classifier2.predict(vectors[len(train_labels):])\n",
        "  print(classification_report(test_labels, labels_pred, digits=3))"
      ],
      "metadata": {
        "id": "0p3uv_GHeyeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc2string(doc):\n",
        "  return \" \".join([w for sent in doc for w in sent])"
      ],
      "metadata": {
        "id": "KCS54YCW2vdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VADER Baseline**"
      ],
      "metadata": {
        "id": "gAm4UxVFiu4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse complete review\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer, VaderConstants\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "labels = numpy.array([0] * len(neg) + [1] * len(pos))\n",
        "prediction_val = [analyzer.polarity_scores(doc2string(v)) for v in (pos + neg)]\n",
        "prediction_labels = [0 if p['pos'] > p['neg'] else 1 for p in prediction_val]\n",
        "\n",
        "print(classification_report(labels, prediction_labels, digits=3))"
      ],
      "metadata": {
        "id": "EPUcrVFXkblr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse each sentence of review, sum sentences contribution as 1\n",
        "prediction_labels = []\n",
        "\n",
        "for rev in (pos+neg):\n",
        "  pos_ = 0\n",
        "  neg_ = 0\n",
        "  for sent in rev:\n",
        "    p = analyzer.polarity_scores(\" \".join([w for w in sent]))\n",
        "    if p['pos'] > p['neg']: pos_ += 1\n",
        "    else: neg_ += 1\n",
        "  prediction_labels.append(0 if pos_ > neg_ else 1)\n",
        "print(classification_report(labels, prediction_labels, digits=3))\n",
        "\n",
        "# Analyse each sentence of review, sum sentences contribution as 1\n",
        "prediction_labels = []\n",
        "\n",
        "for rev in (pos+neg):\n",
        "  pos_ = 0\n",
        "  neg_ = 0\n",
        "  for sent in rev:\n",
        "    p = analyzer.polarity_scores(\" \".join([w for w in sent]))\n",
        "    if p['pos'] > p['neg']: pos_ += p['pos']\n",
        "    else: neg_ += p['neg']\n",
        "  prediction_labels.append(0 if pos_ > neg_ else 1)\n",
        "print(classification_report(labels, prediction_labels, digits=3))"
      ],
      "metadata": {
        "id": "JGLM9GGPSr4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}