{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNG5KG23CuBdPOuIvYEwBKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidemichelon11/NLU/blob/main/NLU_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfoDANvvOpeO"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import subjectivity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('subjectivity')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**BASELINE SUBJECTIVITY**\n",
        "\n"
      ],
      "metadata": {
        "id": "j8oBaoSAM_HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "id": "hDlPJeFxfzpH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc2string(doc):\n",
        "  return \" \".join([w for sent in doc for w in sent])\n",
        "\n",
        "def sent2string(sent):\n",
        "  return \" \".join([w for w in sent])"
      ],
      "metadata": {
        "id": "KCS54YCW2vdT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "classifier_NB = MultinomialNB()\n",
        "\n",
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')]\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')]\n",
        "\n",
        "corpus = [sent2string(d[0]).lower() for d in subj_docs] + [sent2string(d[0]).lower() for d in obj_docs]\n",
        "vectors = vectorizer.fit_transform(corpus)\n",
        "\n",
        "labels = np.array(['subj'] * len(subj_docs) + ['obj'] * len(obj_docs))\n",
        "scores = cross_validate(classifier_NB, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
        "average = sum(scores['test_f1_micro'])/len(scores['test_f1_micro'])\n",
        "print(round(average, 3))"
      ],
      "metadata": {
        "id": "XQduyIdBM-o6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f543b9e6-d891-45b9-a7a4-b6cd590f897f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NB and SVM for subj\n",
        "classifier_NB2_subj = MultinomialNB()\n",
        "classifier_SVM_subj = svm.SVC()\n",
        "\n",
        "corpus = [sent2string(d[0]).lower() for d in subj_docs] + [sent2string(d[0]).lower() for d in obj_docs]\n",
        "labels = np.array(['subj'] * len(subj_docs) + ['obj'] * len(obj_docs))\n",
        "train_samples, test_samples, train_labels, test_labels = train_test_split(corpus, labels, test_size=0.3)\n",
        "vectors = vectorizer.fit_transform(train_samples + test_samples)\n",
        "\n",
        "classifier_NB2_subj.fit(vectors[:len(train_samples)], train_labels)\n",
        "labels_pred_NB2 = classifier_NB2_subj.predict(vectors[len(train_labels):])\n",
        "print(classification_report(test_labels, labels_pred_NB2, digits=3))\n",
        "\n",
        "classifier_SVM_subj.fit(vectors[:len(train_samples)], train_labels)\n",
        "labels_pred_SVM = classifier_SVM_subj.predict(vectors[len(train_labels):])\n",
        "print(classification_report(test_labels, labels_pred_SVM, digits=3))"
      ],
      "metadata": {
        "id": "lDE_J5Wjx1V3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b78cb9-0b02-44bd-8022-fb115d1c4edf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         obj      0.934     0.916     0.925      1494\n",
            "        subj      0.918     0.936     0.927      1506\n",
            "\n",
            "    accuracy                          0.926      3000\n",
            "   macro avg      0.926     0.926     0.926      3000\n",
            "weighted avg      0.926     0.926     0.926      3000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         obj      0.889     0.884     0.887      1494\n",
            "        subj      0.886     0.890     0.888      1506\n",
            "\n",
            "    accuracy                          0.887      3000\n",
            "   macro avg      0.887     0.887     0.887      3000\n",
            "weighted avg      0.887     0.887     0.887      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BASELINE via SVM - SA**"
      ],
      "metadata": {
        "id": "IhxZwaOvaBcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "mr = movie_reviews\n",
        "neg = mr.paras(categories='neg')\n",
        "pos = mr.paras(categories='pos')"
      ],
      "metadata": {
        "id": "NGeoX7znxAbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe76694e-bdac-45fb-8feb-88a22ddb75f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer2 = CountVectorizer()\n",
        "classifier_sa = svm.SVC()\n",
        "\n",
        "corpus = [doc2string(p) for p in pos] + [doc2string(n) for n in neg]\n",
        "labels = np.array([0] * len(pos) + [1] * len(neg))\n",
        "train_samples, test_samples, train_labels, test_labels = train_test_split(corpus, labels, test_size=0.1)\n",
        "\n",
        "vectors = vectorizer2.fit_transform(train_samples + test_samples)\n",
        "classifier_sa.fit(vectors[:len(train_samples)], train_labels)\n",
        "labels_pred = classifier_sa.predict(vectors[len(train_labels):])\n",
        "\n",
        "print(classification_report(test_labels, labels_pred, digits=3))"
      ],
      "metadata": {
        "id": "Cnx3fdGhcYKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62b753e-67d2-4ed4-d932-24ed4dedba90"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.824     0.680     0.745       103\n",
            "           1      0.713     0.845     0.774        97\n",
            "\n",
            "    accuracy                          0.760       200\n",
            "   macro avg      0.768     0.762     0.759       200\n",
            "weighted avg      0.770     0.760     0.759       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each review, remove obj sentences and compute the SVM\n",
        "vectorizer3 = CountVectorizer()\n",
        "classifier_sa2 = svm.SVC()\n",
        "\n",
        "def get_new_rev(original):\n",
        "  new_list = []\n",
        "  for rev in original:\n",
        "    new_rev = []\n",
        "    for s in rev:\n",
        "      vector = vectorizer.transform([sent2string(s)]).toarray()\n",
        "      if classifier_NB2_subj.predict(vector) == ['subj']: \n",
        "        new_rev.append(s)\n",
        "    new_list.append(new_rev)\n",
        "  return new_list\n",
        "              \n",
        "new_pos = get_new_rev(pos)\n",
        "new_neg = get_new_rev(neg)\n",
        "\n",
        "corpus_ = [doc2string(p) for p in new_pos] + [doc2string(n) for n in new_neg]\n",
        "labels_ = np.array([0] * len(new_pos) + [1] * len(new_neg))\n",
        "train_samples_, test_samples_, train_labels_, test_labels_ = train_test_split(corpus_, labels_, test_size=0.1)\n",
        "\n",
        "vectors_ = vectorizer3.fit_transform(train_samples_ + test_samples_)\n",
        "classifier_sa2.fit(vectors_[:len(train_samples_)], train_labels_)\n",
        "labels_pred_ = classifier_sa2.predict(vectors_[len(train_labels_):])\n",
        "\n",
        "print(classification_report(test_labels_, labels_pred_, digits=3))"
      ],
      "metadata": {
        "id": "vLGRR3-Q7nuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb18076c-2950-4c91-fb46-b5c509a39891"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.792     0.792     0.792       101\n",
            "           1      0.788     0.788     0.788        99\n",
            "\n",
            "    accuracy                          0.790       200\n",
            "   macro avg      0.790     0.790     0.790       200\n",
            "weighted avg      0.790     0.790     0.790       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VADER Baseline**"
      ],
      "metadata": {
        "id": "gAm4UxVFiu4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer, VaderConstants"
      ],
      "metadata": {
        "id": "6f80ZdAHf5gU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse complete review\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "labels_vader = np.array([0] * len(neg) + [1] * len(pos))\n",
        "prediction_val = [analyzer.polarity_scores(doc2string(v)) for v in (pos + neg)]\n",
        "prediction_labels = [0 if p['pos'] > p['neg'] else 1 for p in prediction_val]\n",
        "\n",
        "print(classification_report(labels_vader, prediction_labels, digits=3))"
      ],
      "metadata": {
        "id": "EPUcrVFXkblr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d499779-20e9-4467-a7f1-b2e9f9545394"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.583     0.842     0.689      1000\n",
            "           1      0.715     0.397     0.511      1000\n",
            "\n",
            "    accuracy                          0.620      2000\n",
            "   macro avg      0.649     0.619     0.600      2000\n",
            "weighted avg      0.649     0.620     0.600      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse each sentence of review, sum sentences contribution as 1\n",
        "prediction_labels = []\n",
        "\n",
        "for rev in (pos+neg):\n",
        "  pos_ = 0\n",
        "  neg_ = 0\n",
        "  for sent in rev:\n",
        "    p = analyzer.polarity_scores(\" \".join([w for w in sent]))\n",
        "    if p['pos'] > p['neg']: pos_ += 1\n",
        "    else: neg_ += 1\n",
        "  prediction_labels.append(0 if pos_ > neg_ else 1)\n",
        "print(classification_report(labels_vader, prediction_labels, digits=3))\n",
        "\n",
        "prediction_labels = []\n",
        "\n",
        "for rev in (pos+neg):\n",
        "  pos_ = 0\n",
        "  neg_ = 0\n",
        "  for sent in rev:\n",
        "    p = analyzer.polarity_scores(\" \".join([w for w in sent]))\n",
        "    if p['pos'] > p['neg']: pos_ += p['pos']\n",
        "    else: neg_ += p['neg']\n",
        "  prediction_labels.append(0 if pos_ > neg_ else 1)\n",
        "print(classification_report(labels, prediction_labels, digits=3))"
      ],
      "metadata": {
        "id": "JGLM9GGPSr4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7111d9cf-bc29-4679-d14d-f9d665fba31f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.698     0.500     0.583      1000\n",
            "           1      0.611     0.784     0.687      1000\n",
            "\n",
            "    accuracy                          0.642      2000\n",
            "   macro avg      0.654     0.642     0.635      2000\n",
            "weighted avg      0.654     0.642     0.635      2000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.602     0.843     0.702      1000\n",
            "           1      0.738     0.442     0.553      1000\n",
            "\n",
            "    accuracy                          0.642      2000\n",
            "   macro avg      0.670     0.642     0.628      2000\n",
            "weighted avg      0.670     0.642     0.628      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If time, another baseline on ['neu']\n",
        "Aggiungere _NEG anche baseline"
      ],
      "metadata": {
        "id": "di88rSOyO-OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-agqb7wb0Jf8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "    c for c in unicodedata.normalize('NFD', s)\n",
        "    if unicodedata.category(c) != 'Mn'\n",
        "  )\n",
        "\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "  return s"
      ],
      "metadata": {
        "id": "FUZ1SSoT9QBr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def create_tens(rev, seq_len, word_embedding, doc):\n",
        "\n",
        "  rev = normalizeString(rev).split()\n",
        "  if len(rev) > seq_len: rev = rev[:seq_len]\n",
        "  # rev2Tens = torch.zeros(seq_len, word_embedding)\n",
        "  sent = []\n",
        "  for i, w in enumerate(rev):\n",
        "    vector = nlp.vocab[w].vector\n",
        "    sent.append(vector)\n",
        "\n",
        "  z = list(np.zeros(word_embedding, dtype=np.float32))\n",
        "  if len(sent) > seq_len:\n",
        "    return sent[:seq_len]\n",
        "  else:\n",
        "    diff = seq_len - len(sent)\n",
        "    zs = [z for each in range(diff)]\n",
        "    return zs + sent"
      ],
      "metadata": {
        "id": "rpnC4z92-Cki"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjDataset (data.Dataset):\n",
        "  def __init__(self, rev, labels):\n",
        "    self.rev = rev\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.rev)\n",
        "\n",
        "  def __getitem__(self, idx: int):\n",
        "    return torch.tensor(self.rev[idx]), torch.tensor(self.labels[idx])"
      ],
      "metadata": {
        "id": "F_9zUQ9luN_U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.cli.download('en_core_web_lg')\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "nlp.max_length = 1300000\n",
        "\n",
        "text = \"\"\n",
        "corpus = [sent2string(d[0]).lower() for d in subj_docs] + [sent2string(d[0]).lower() for d in obj_docs]\n",
        "for sent in corpus:\n",
        "  sent = normalizeString(sent)\n",
        "  text = text + \" \"+ sent\n",
        "\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "sKBnf2FS-GwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 120\n",
        "seq_len = 100\n",
        "word_embedding = 300\n",
        "\n",
        "labels = np.append(np.zeros((len(subj_docs)), dtype=int), np.ones((len(subj_docs)), dtype=int))\n",
        "\n",
        "train_samples, test_samples, train_labels, test_labels = train_test_split(corpus, labels, test_size=0.3)\n",
        "\n",
        "train_samples = [create_tens(rev, seq_len, word_embedding, doc) for rev in train_samples]\n",
        "test_samples = [create_tens(rev, seq_len, word_embedding, doc) for rev in test_samples]\n",
        "\n",
        "# train samples are tensors of seq_len x word_embedding\n",
        "train_dataset = ObjDataset(train_samples, train_labels)\n",
        "test_dataset = ObjDataset(test_samples, test_labels)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,)"
      ],
      "metadata": {
        "id": "4yYFIJRRPEKt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    \n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    \n",
        "    self.i2h = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    self.i2o = nn.Linear(hidden_size, output_size)\n",
        "  \n",
        "  def forward(self, input, hidden=None):\n",
        "    if hidden==None:\n",
        "      hidden = self.init_hidden(input.shape[0])\n",
        "    output, _ = self.i2h(input, hidden)\n",
        "    output = self.i2o(output[:, -1])\n",
        "    return output\n",
        "\n",
        "  def init_hidden(self,shape=1):\n",
        "    return torch.zeros(1, shape, self.hidden_size)"
      ],
      "metadata": {
        "id": "XkhcSBLc2c-7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "def train(rnn, optimizer, train_loader):\n",
        "\n",
        "  cumulative_accuracy = 0\n",
        "  samples=0\n",
        "\n",
        "  for x,y in train_loader:\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    outputs = rnn(x)\n",
        "    loss = criterion(outputs, y.long())\n",
        "    _, predicted = outputs.max(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    samples += x.shape[1]\n",
        "  \n",
        "    cumulative_accuracy += predicted.eq(y).sum().item()\n",
        "  return cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "-o1ey6Xa3fyl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(rnn, test_loader):\n",
        "\n",
        "  cumulative_accuracy = 0\n",
        "  samples=0\n",
        "  for x,y in test_loader:\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    outputs = rnn(x)\n",
        "    loss = criterion(outputs, y.long())\n",
        "    _, predicted = outputs.max(1)\n",
        "    samples += x.shape[1]\n",
        "    cumulative_accuracy += predicted.eq(y).sum().item()\n",
        "  return cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "ZCRFmwhhmvLC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "learning_rate = 0.005\n",
        "n_hidden = 128\n",
        "n_categories = 2\n",
        "\n",
        "rnn = RNN(word_embedding, n_hidden, n_categories)\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "test_accuracy = evaluate(rnn, test_loader)\n",
        "print('Before training, test accuracy: {:.2f}'.format(test_accuracy))\n",
        "\n",
        "for e in range(epochs):\n",
        "  train_accuracy = train(rnn, optimizer, train_loader)\n",
        "  print('Epoch: {}/{} Train accuracy: {:.2f}'.format(e+1, epochs, train_accuracy))\n",
        "\n",
        "test_accuracy = evaluate(rnn, test_loader)\n",
        "print('After training, test accuracy: {:.2f}'.format(test_accuracy))\n"
      ],
      "metadata": {
        "id": "rzxNncVo4KV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1bbbad-4693-4570-e932-9f4467d2f1ad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training, test accuracy: 59.56\n",
            "Epoch: 1/10 Train accuracy: 59.58\n",
            "Epoch: 2/10 Train accuracy: 64.93\n",
            "Epoch: 3/10 Train accuracy: 70.10\n",
            "Epoch: 4/10 Train accuracy: 73.95\n",
            "Epoch: 5/10 Train accuracy: 76.71\n",
            "Epoch: 6/10 Train accuracy: 79.25\n",
            "Epoch: 7/10 Train accuracy: 81.64\n",
            "Epoch: 8/10 Train accuracy: 83.85\n",
            "Epoch: 9/10 Train accuracy: 86.37\n",
            "Epoch: 10/10 Train accuracy: 91.73\n",
            "After training, test accuracy: 98.88\n"
          ]
        }
      ]
    }
  ]
}