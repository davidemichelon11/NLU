{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPY/CjHO4Q0tO7de4DQX71L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidemichelon11/NLU/blob/main/NLU_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfoDANvvOpeO",
        "outputId": "2b31b695-81a1-4cb5-a787-1b16992602d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Package subjectivity is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import subjectivity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('subjectivity')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**BASELINE SUBJECTIVITY**\n",
        "\n"
      ],
      "metadata": {
        "id": "j8oBaoSAM_HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "id": "hDlPJeFxfzpH"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc2string(doc):\n",
        "  return \" \".join([w for sent in doc for w in sent])\n",
        "\n",
        "def sent2string(sent):\n",
        "  return \" \".join([w for w in sent])"
      ],
      "metadata": {
        "id": "KCS54YCW2vdT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "classifier_NB = MultinomialNB()\n",
        "\n",
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')]\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')]\n",
        "\n",
        "corpus = [sent2string(d[0]).lower() for d in subj_docs] + [sent2string(d[0]).lower() for d in obj_docs]\n",
        "vectors = vectorizer.fit_transform(corpus)\n",
        "\n",
        "labels = np.array(['subj'] * len(subj_docs) + ['obj'] * len(obj_docs))\n",
        "scores = cross_validate(classifier_NB, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
        "average = sum(scores['test_f1_micro'])/len(scores['test_f1_micro'])\n",
        "print(round(average, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQduyIdBM-o6",
        "outputId": "f3d2d1a5-2994-44db-878a-31dce7d8652d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NB and SVM for subj\n",
        "classifier_NB2_subj = MultinomialNB()\n",
        "classifier_SVM_subj = svm.SVC()\n",
        "\n",
        "corpus = [sent2string(d[0]).lower() for d in subj_docs] + [sent2string(d[0]).lower() for d in obj_docs]\n",
        "labels = np.array(['subj'] * len(subj_docs) + ['obj'] * len(obj_docs))\n",
        "train_samples, test_samples, train_labels, test_labels = train_test_split(corpus, labels, test_size=0.3)\n",
        "vectors = vectorizer.fit_transform(train_samples + test_samples)\n",
        "\n",
        "classifier_NB2_subj.fit(vectors[:len(train_samples)], train_labels)\n",
        "labels_pred_NB2 = classifier_NB2_subj.predict(vectors[len(train_labels):])\n",
        "print(classification_report(test_labels, labels_pred_NB2, digits=3))\n",
        "\n",
        "classifier_SVM_subj.fit(vectors[:len(train_samples)], train_labels)\n",
        "labels_pred_SVM = classifier_SVM_subj.predict(vectors[len(train_labels):])\n",
        "print(classification_report(test_labels, labels_pred_SVM, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDE_J5Wjx1V3",
        "outputId": "4c17cf8c-c92f-4146-d767-ad3b13749b27"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         obj      0.933     0.905     0.919      1516\n",
            "        subj      0.906     0.934     0.920      1484\n",
            "\n",
            "    accuracy                          0.919      3000\n",
            "   macro avg      0.920     0.919     0.919      3000\n",
            "weighted avg      0.920     0.919     0.919      3000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         obj      0.894     0.872     0.883      1516\n",
            "        subj      0.873     0.895     0.884      1484\n",
            "\n",
            "    accuracy                          0.883      3000\n",
            "   macro avg      0.883     0.883     0.883      3000\n",
            "weighted avg      0.884     0.883     0.883      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BASELINE via SVM - SA**"
      ],
      "metadata": {
        "id": "IhxZwaOvaBcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "mr = movie_reviews\n",
        "neg = mr.paras(categories='neg')\n",
        "pos = mr.paras(categories='pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGeoX7znxAbV",
        "outputId": "10031fd3-e85b-41d2-b63b-0723317529a6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer2 = CountVectorizer()\n",
        "classifier_sa = svm.SVC()\n",
        "\n",
        "corpus = [doc2string(p) for p in pos] + [doc2string(n) for n in neg]\n",
        "labels = np.array([0] * len(pos) + [1] * len(neg))\n",
        "train_samples, test_samples, train_labels, test_labels = train_test_split(corpus, labels, test_size=0.1)\n",
        "\n",
        "vectors = vectorizer2.fit_transform(train_samples + test_samples)\n",
        "classifier_sa.fit(vectors[:len(train_samples)], train_labels)\n",
        "labels_pred = classifier_sa.predict(vectors[len(train_labels):])\n",
        "\n",
        "print(classification_report(test_labels, labels_pred, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnx3fdGhcYKX",
        "outputId": "eff25898-6d63-4007-cc3f-a0602479aafa"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.759     0.670     0.712        94\n",
            "           1      0.735     0.811     0.771       106\n",
            "\n",
            "    accuracy                          0.745       200\n",
            "   macro avg      0.747     0.741     0.742       200\n",
            "weighted avg      0.746     0.745     0.743       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each review, remove obj sentences and compute the SVM\n",
        "vectorizer3 = CountVectorizer()\n",
        "classifier_sa2 = svm.SVC()\n",
        "\n",
        "def get_new_rev(original):\n",
        "  new_list = []\n",
        "  for rev in original:\n",
        "    new_rev = []\n",
        "    for s in rev:\n",
        "      vector = vectorizer.transform([sent2string(s)]).toarray()\n",
        "      if classifier_NB2_subj.predict(vector) == ['subj']: \n",
        "        new_rev.append(s)\n",
        "    new_list.append(new_rev)\n",
        "  return new_list\n",
        "              \n",
        "new_pos = get_new_rev(pos)\n",
        "new_neg = get_new_rev(neg)\n",
        "\n",
        "corpus_ = [doc2string(p) for p in new_pos] + [doc2string(n) for n in new_neg]\n",
        "labels_ = np.array([0] * len(new_pos) + [1] * len(new_neg))\n",
        "train_samples_, test_samples_, train_labels_, test_labels_ = train_test_split(corpus_, labels_, test_size=0.1)\n",
        "\n",
        "vectors_ = vectorizer3.fit_transform(train_samples_ + test_samples_)\n",
        "classifier_sa2.fit(vectors_[:len(train_samples_)], train_labels_)\n",
        "labels_pred_ = classifier_sa2.predict(vectors_[len(train_labels_):])\n",
        "\n",
        "print(classification_report(test_labels_, labels_pred_, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLGRR3-Q7nuS",
        "outputId": "52d59ed1-1778-427b-a89c-5daf97d1783a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.825     0.748     0.784       107\n",
            "           1      0.738     0.817     0.776        93\n",
            "\n",
            "    accuracy                          0.780       200\n",
            "   macro avg      0.781     0.782     0.780       200\n",
            "weighted avg      0.784     0.780     0.780       200\n",
            "\n",
            "accuracy:  0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VADER Baseline**"
      ],
      "metadata": {
        "id": "gAm4UxVFiu4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer, VaderConstants"
      ],
      "metadata": {
        "id": "6f80ZdAHf5gU"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse complete review\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "labels_vader = np.array([0] * len(neg) + [1] * len(pos))\n",
        "prediction_val = [analyzer.polarity_scores(doc2string(v)) for v in (pos + neg)]\n",
        "prediction_labels = [0 if p['pos'] > p['neg'] else 1 for p in prediction_val]\n",
        "\n",
        "print(classification_report(labels_vader, prediction_labels, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPUcrVFXkblr",
        "outputId": "9a7d6776-30d1-4ec9-ce52-d69c853e58a1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.583     0.842     0.689      1000\n",
            "           1      0.715     0.397     0.511      1000\n",
            "\n",
            "    accuracy                          0.620      2000\n",
            "   macro avg      0.649     0.619     0.600      2000\n",
            "weighted avg      0.649     0.620     0.600      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse each sentence of review, sum sentences contribution as 1\n",
        "prediction_labels = []\n",
        "\n",
        "for rev in (pos+neg):\n",
        "  pos_ = 0\n",
        "  neg_ = 0\n",
        "  for sent in rev:\n",
        "    p = analyzer.polarity_scores(\" \".join([w for w in sent]))\n",
        "    if p['pos'] > p['neg']: pos_ += 1\n",
        "    else: neg_ += 1\n",
        "  prediction_labels.append(0 if pos_ > neg_ else 1)\n",
        "print(classification_report(labels_vader, prediction_labels, digits=3))\n",
        "\n",
        "prediction_labels = []\n",
        "\n",
        "for rev in (pos+neg):\n",
        "  pos_ = 0\n",
        "  neg_ = 0\n",
        "  for sent in rev:\n",
        "    p = analyzer.polarity_scores(\" \".join([w for w in sent]))\n",
        "    if p['pos'] > p['neg']: pos_ += p['pos']\n",
        "    else: neg_ += p['neg']\n",
        "  prediction_labels.append(0 if pos_ > neg_ else 1)\n",
        "print(classification_report(labels, prediction_labels, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGLM9GGPSr4h",
        "outputId": "3b7c403c-09bc-435e-d79f-f8308fb0572f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.698     0.500     0.583      1000\n",
            "           1      0.611     0.784     0.687      1000\n",
            "\n",
            "    accuracy                          0.642      2000\n",
            "   macro avg      0.654     0.642     0.635      2000\n",
            "weighted avg      0.654     0.642     0.635      2000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.602     0.843     0.702      1000\n",
            "           1      0.738     0.442     0.553      1000\n",
            "\n",
            "    accuracy                          0.642      2000\n",
            "   macro avg      0.670     0.642     0.628      2000\n",
            "weighted avg      0.670     0.642     0.628      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If time, another baseline on ['neu']\n",
        "Aggiungere _NEG anche baseline"
      ],
      "metadata": {
        "id": "di88rSOyO-OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-agqb7wb0Jf8"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "# scroll all reviews and create matrix [n_reviews x n_words]\n",
        "class Rev2Vec:\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.n_words = 0\n",
        "\n",
        "  def addRev(self, rev):\n",
        "    for word in rev.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "    c for c in unicodedata.normalize('NFD', s)\n",
        "    if unicodedata.category(c) != 'Mn'\n",
        "  )\n",
        "\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "  return s\n",
        "\n",
        "def prepare_data(reviews: list):\n",
        "  voc = Rev2Vec()\n",
        "  for rev in reviews:\n",
        "    voc.addRev(normalizeString(rev))\n",
        "  return voc\n",
        "\n",
        "def create_dataset(v, reviews):\n",
        "  revs2Tens = torch.zeros(len(reviews), v.n_words)\n",
        "  for i, rev in enumerate(reviews):\n",
        "    word_emb = []\n",
        "    rev_arr = normalizeString(rev).split()\n",
        "    for w in rev_arr:\n",
        "      revs2Tens[ i,v.word2index[str(w)]] += 1\n",
        "  return revs2Tens"
      ],
      "metadata": {
        "id": "FUZ1SSoT9QBr"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjDataset (data.Dataset):\n",
        "  def __init__(self, rev, labels):\n",
        "    self.rev = rev\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.rev)\n",
        "\n",
        "  def __getitem__(self, idx: int):\n",
        "    return self.rev[idx], torch.tensor(self.labels[idx])"
      ],
      "metadata": {
        "id": "F_9zUQ9luN_U"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "corpus = [sent2string(d[0]).lower() for d in subj_docs] + [sent2string(d[0]).lower() for d in obj_docs]\n",
        "labels = np.append(np.zeros((len(subj_docs)), dtype=int), np.ones((len(subj_docs)), dtype=int))\n",
        "\n",
        "dataset_tor = create_dataset(v, corpus)\n",
        "train_samples, test_samples, train_labels, test_labels = train_test_split(dataset_tor, labels, test_size=0.3)\n",
        "train_dataset = ObjDataset(train_samples, train_labels)\n",
        "test_dataset = ObjDataset(test_samples, test_labels)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size)"
      ],
      "metadata": {
        "id": "4yYFIJRRPEKt"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    \n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    \n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    combined = torch.cat((input, hidden), 1)\n",
        "    hidden = self.i2h(combined)\n",
        "    output = self.i2o(combined)\n",
        "    output = self.softmax(output)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self,shape=1):\n",
        "    return torch.zeros(shape, self.hidden_size)\n",
        "\n",
        "class RNNcat(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
        "    self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
        "    self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, category, input, hidden):\n",
        "    input_combined = torch.cat((category, input, hidden), 1)\n",
        "    hidden = self.i2h(input_combined)\n",
        "    output = self.i2o(input_combined)\n",
        "    output_combined = torch.cat((hidden, output), 1)\n",
        "    output = self.o2o(output_combined)\n",
        "    output = self.dropout(output)\n",
        "    output = self.softmax(output)\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, self.hidden_size)\n",
        "# class RNN(nn.Module):\n",
        "#   def __init__(self, input_size, hidden_size, output_size):\n",
        "#     super(RNN, self).__init__()\n",
        "    \n",
        "#     self.input_size = input_size\n",
        "#     self.hidden_size = hidden_size\n",
        "#     self.output_size = output_size\n",
        "    \n",
        "#     self.i2h = nn.RNN(input_size, hidden_size)\n",
        "#     self.i2o = nn.Linear(hidden_size, output_size)\n",
        "  \n",
        "#   def forward(self, input, hidden=None):\n",
        "#     if hidden==None:\n",
        "#       hidden = self.init_hidden(input.shape[1])\n",
        "#     print(input.shape)\n",
        "#     print(hidden.shape)\n",
        "#     output, _ = self.i2h(input, hidden)\n",
        "#     output = self.i2o(output[-1])\n",
        "#     return output\n",
        "\n",
        "#   def init_hidden(self,shape=1):\n",
        "#     return torch.zeros(shape, self.hidden_size)"
      ],
      "metadata": {
        "id": "XkhcSBLc2c-7"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "def train(rnn, optimizer, train_loader, hidden):\n",
        "\n",
        "  cumulative_accuracy = 0\n",
        "  samples=0\n",
        "\n",
        "  for x,y in train_loader:\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    outputs, next_hidden = rnn(x, hidden)\n",
        "    loss = criterion(outputs, y)\n",
        "    _, predicted = outputs.max(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    samples += x.shape[0]\n",
        "    cumulative_accuracy += predicted.eq(y).sum().item()\n",
        "  return cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "-o1ey6Xa3fyl"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(rnn, test_loader, hidden):\n",
        "\n",
        "  cumulative_accuracy = 0\n",
        "  samples=0\n",
        "\n",
        "  for x,y in test_loader:\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    outputs, next_hidden = rnn(x, hidden)\n",
        "    loss = criterion(outputs, y)\n",
        "    _, predicted = outputs.max(1)\n",
        "    samples += x.shape[0]\n",
        "    cumulative_accuracy += predicted.eq(y).sum().item()\n",
        "  return cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "ZCRFmwhhmvLC"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "learning_rate = 0.005\n",
        "n_hidden = 128\n",
        "n_categories = 2\n",
        "current_loss = 0\n",
        "\n",
        "hidden = torch.zeros(batch_size, n_hidden)\n",
        "n_features = v.n_words\n",
        "rnn = RNN(n_features, n_hidden, n_categories)\n",
        "rnn_cat = RNNcat((n_features, n_hidden, n_categories))\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "#Evaluation before training\n",
        "test_accuracy = evaluate(rnn, test_loader, hidden)\n",
        "print('Before training, test accuracy: {:.2f}'.format(test_accuracy))\n",
        "for e in range(epochs):\n",
        "  train_accuracy = train(rnn, optimizer, train_loader, hidden)\n",
        "#Evaluation after testing\n",
        "test_accuracy = evaluate(rnn, test_loader, hidden)\n",
        "print('After training, test accuracy: {:.2f}'.format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzxNncVo4KV7",
        "outputId": "a612cb04-0e95-4ce7-c729-0814f0fc4d38"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training, test accuracy: 48.00\n",
            "After training, test accuracy: 78.80\n"
          ]
        }
      ]
    }
  ]
}