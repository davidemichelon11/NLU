{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLU_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMVtJ2fibUjU0Aes2tf6+d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidemichelon11/NLU/blob/main/NLU_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dfoDANvvOpeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45381ad-d382-4679-b39f-598b310ccb69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Package subjectivity is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.corpus import subjectivity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('subjectivity')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**BASELINE SUBJECTIVITY**\n",
        "\n"
      ],
      "metadata": {
        "id": "j8oBaoSAM_HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "id": "hDlPJeFxfzpH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc2string(doc):\n",
        "  return \" \".join([w for sent in doc for w in sent])\n",
        "\n",
        "def sent2string(sent):\n",
        "  return \" \".join([w for w in sent])"
      ],
      "metadata": {
        "id": "KCS54YCW2vdT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "    c for c in unicodedata.normalize('NFD', s)\n",
        "    if unicodedata.category(c) != 'Mn'\n",
        "  )\n",
        "\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "  return s"
      ],
      "metadata": {
        "id": "EI38_yFDSR6s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')]\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')]\n",
        "\n",
        "corpus_sub = [normalizeString(sent2string(d[0])) for d in subj_docs] + [normalizeString(sent2string(d[0])) for d in obj_docs]\n",
        "labels_sub = np.array(['subj'] * len(subj_docs) + ['obj'] * len(obj_docs))\n",
        "train_samples_sub, test_samples_sub, train_labels_sub, test_labels_sub = train_test_split(corpus_sub, labels_sub, test_size=0.3)"
      ],
      "metadata": {
        "id": "1yfWeEvbJLeg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_sub = CountVectorizer()\n",
        "classifier_NB = MultinomialNB()\n",
        "\n",
        "vectors = vectorizer_sub.fit_transform(corpus_sub)\n",
        "scores = cross_validate(classifier_NB, vectors, labels_sub, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
        "average = sum(scores['test_f1_micro'])/len(scores['test_f1_micro'])\n",
        "print(round(average, 3))"
      ],
      "metadata": {
        "id": "XQduyIdBM-o6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc66b06-15e2-4577-b0da-5e1fd6da8e75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NB and SVM for subj\n",
        "classifier_NB2_subj = MultinomialNB()\n",
        "classifier_SVM_subj = svm.SVC()\n",
        "\n",
        "vectors = vectorizer_sub.fit_transform(train_samples_sub + test_samples_sub)\n",
        "\n",
        "classifier_NB2_subj.fit(vectors[:len(train_samples_sub)], train_labels_sub)\n",
        "labels_pred_NB2 = classifier_NB2_subj.predict(vectors[len(train_labels_sub):])\n",
        "print(classification_report(test_labels_sub, labels_pred_NB2, digits=3))\n",
        "\n",
        "#SVM\n",
        "classifier_SVM_subj.fit(vectors[:len(train_samples_sub)], train_labels_sub)\n",
        "labels_pred_SVM = classifier_SVM_subj.predict(vectors[len(train_labels_sub):])\n",
        "print(classification_report(test_labels_sub, labels_pred_SVM, digits=3))"
      ],
      "metadata": {
        "id": "lDE_J5Wjx1V3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d294ab-0da9-4a94-d810-03ecc1a12109"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         obj      0.928     0.900     0.914      1472\n",
            "        subj      0.906     0.933     0.919      1528\n",
            "\n",
            "    accuracy                          0.917      3000\n",
            "   macro avg      0.917     0.916     0.917      3000\n",
            "weighted avg      0.917     0.917     0.917      3000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         obj      0.883     0.882     0.883      1472\n",
            "        subj      0.887     0.887     0.887      1528\n",
            "\n",
            "    accuracy                          0.885      3000\n",
            "   macro avg      0.885     0.885     0.885      3000\n",
            "weighted avg      0.885     0.885     0.885      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BASELINE via SVM - SA**"
      ],
      "metadata": {
        "id": "IhxZwaOvaBcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "mr = movie_reviews\n",
        "neg = mr.paras(categories='neg')\n",
        "pos = mr.paras(categories='pos')"
      ],
      "metadata": {
        "id": "NGeoX7znxAbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07247a04-6c4e-47ea-b514-68d1b0e94e84"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_sa = CountVectorizer()\n",
        "classifier_sa = svm.SVC()\n",
        "\n",
        "#each element is the review converted to string\n",
        "corpus_sa = [normalizeString(doc2string(p)) for p in pos] + [normalizeString(doc2string(n)) for n in neg]\n",
        "\n",
        "labels_sa = np.array([0] * len(pos) + [1] * len(neg))\n",
        "train_samples_sa, test_samples_sa, train_labels_sa, test_labels_sa = train_test_split(corpus_sa, labels_sa, test_size=0.1)"
      ],
      "metadata": {
        "id": "ewUNWUOvKXPy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM with objective sentences\n",
        "vectors = vectorizer_sa.fit_transform(train_samples_sa + test_samples_sa)\n",
        "classifier_sa.fit(vectors[:len(train_samples_sa)], train_labels_sa)\n",
        "labels_pred = classifier_sa.predict(vectors[len(train_labels_sa):])\n",
        "\n",
        "print(classification_report(test_labels_sa, labels_pred, digits=3))"
      ],
      "metadata": {
        "id": "Cnx3fdGhcYKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d83e66e-b2bb-4608-b808-3fb9a6762fed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.821     0.639     0.719       108\n",
            "           1      0.664     0.837     0.740        92\n",
            "\n",
            "    accuracy                          0.730       200\n",
            "   macro avg      0.743     0.738     0.730       200\n",
            "weighted avg      0.749     0.730     0.729       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_obj_sentences(reviews, vectorizer_sub):\n",
        "  new_list = []\n",
        "  for rev in reviews:\n",
        "    new_rev = []\n",
        "    for s in rev:\n",
        "      vector = vectorizer_sub.transform([sent2string(s)]).toarray()\n",
        "      if classifier_NB2_subj.predict(vector) == ['subj']: \n",
        "        new_rev.append(s)\n",
        "    new_list.append(new_rev)\n",
        "  return new_list"
      ],
      "metadata": {
        "id": "i0fw_ujaMF8V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM without obj sentences --> circa 80-82% accuracy\n",
        "# For each review, remove obj sentences and compute the SVM\n",
        "vectorizer_sa_subj = CountVectorizer()\n",
        "classifier_SVM_subj = svm.SVC()\n",
        "     \n",
        "new_pos = remove_obj_sentences(pos, vectorizer_sub)\n",
        "new_neg = remove_obj_sentences(neg, vectorizer_sub)\n",
        "\n",
        "corpus_sa_subj = [normalizeString(doc2string(p)) for p in new_pos] + [normalizeString(doc2string(n)) for n in new_neg]\n",
        "labels_sa_subj = np.array([0] * len(new_pos) + [1] * len(new_neg))\n",
        "train_samples_, test_samples_, train_labels_, test_labels_ = train_test_split(corpus_sa_subj, labels_sa_subj, test_size=0.2)\n",
        "\n",
        "vectors_ = vectorizer_sa_subj.fit_transform(train_samples_ + test_samples_)\n",
        "classifier_SVM_subj.fit(vectors_[:len(train_samples_)], train_labels_)\n",
        "labels_pred_ = classifier_SVM_subj.predict(vectors_[len(train_labels_):])\n",
        "\n",
        "print(classification_report(test_labels_, labels_pred_, digits=3))"
      ],
      "metadata": {
        "id": "vLGRR3-Q7nuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8faa0204-6d9c-4542-9efd-333e2bf9f7c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.808     0.746     0.776       197\n",
            "           1      0.771     0.828     0.798       203\n",
            "\n",
            "    accuracy                          0.787       400\n",
            "   macro avg      0.789     0.787     0.787       400\n",
            "weighted avg      0.789     0.787     0.787       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VADER Baseline**"
      ],
      "metadata": {
        "id": "gAm4UxVFiu4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer, VaderConstants"
      ],
      "metadata": {
        "id": "6f80ZdAHf5gU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse complete review\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "labels_vader = np.array([0] * len(neg) + [1] * len(pos))\n",
        "prediction_val = [analyzer.polarity_scores(doc2string(v)) for v in (pos + neg)]\n",
        "prediction_labels = [0 if p['pos'] > p['neg'] else 1 for p in prediction_val]\n",
        "\n",
        "print(classification_report(labels_sa, prediction_labels, digits=3))"
      ],
      "metadata": {
        "id": "EPUcrVFXkblr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6684532e-f44e-4304-b15e-5f09d8be0811"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.583     0.842     0.689      1000\n",
            "           1      0.715     0.397     0.511      1000\n",
            "\n",
            "    accuracy                          0.620      2000\n",
            "   macro avg      0.649     0.619     0.600      2000\n",
            "weighted avg      0.649     0.620     0.600      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse each sentence of review, sum sentences contribution as 1\n",
        "prediction_labels = []\n",
        "\n",
        "for rev in (pos+neg):\n",
        "  pos_ = 0\n",
        "  neg_ = 0\n",
        "  for sent in rev:\n",
        "    p = analyzer.polarity_scores(\" \".join([w for w in sent]))\n",
        "    if p['pos'] > p['neg']: pos_ += 1\n",
        "    else: neg_ += 1\n",
        "  prediction_labels.append(0 if pos_ > neg_ else 1)\n",
        "print(classification_report(labels_sa, prediction_labels, digits=3))\n",
        "\n",
        "prediction_labels = []\n",
        "\n",
        "for rev in (pos+neg):\n",
        "  pos_ = 0\n",
        "  neg_ = 0\n",
        "  for sent in rev:\n",
        "    p = analyzer.polarity_scores(\" \".join([w for w in sent]))\n",
        "    if p['pos'] > p['neg']: pos_ += p['pos']\n",
        "    else: neg_ += p['neg']\n",
        "  prediction_labels.append(0 if pos_ > neg_ else 1)\n",
        "print(classification_report(labels_sa, prediction_labels, digits=3))"
      ],
      "metadata": {
        "id": "JGLM9GGPSr4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6415ce4-044e-4e08-ed6e-e598a366492d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.698     0.500     0.583      1000\n",
            "           1      0.611     0.784     0.687      1000\n",
            "\n",
            "    accuracy                          0.642      2000\n",
            "   macro avg      0.654     0.642     0.635      2000\n",
            "weighted avg      0.654     0.642     0.635      2000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.602     0.843     0.702      1000\n",
            "           1      0.738     0.442     0.553      1000\n",
            "\n",
            "    accuracy                          0.642      2000\n",
            "   macro avg      0.670     0.642     0.628      2000\n",
            "weighted avg      0.670     0.642     0.628      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectivity detection using a simple RNN and LSTM**"
      ],
      "metadata": {
        "id": "xQLKQjBW62iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-agqb7wb0Jf8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 120\n",
        "seq_len = 50\n",
        "word_embedding = 300\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "n_categories = 2"
      ],
      "metadata": {
        "id": "mY-ln6qufjab"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# before tried to create a vocabulary and 1-hot encoder but dimension of 20k and very big\n",
        "def create_tens(rev, seq_len, word_embedding):\n",
        "  rev = normalizeString(rev).split()\n",
        "  if len(rev) > seq_len: rev = rev[:seq_len]\n",
        "\n",
        "  sent = []\n",
        "  for i, w in enumerate(rev):\n",
        "    vector = nlp.vocab[w].vector\n",
        "    sent.append(vector)\n",
        "\n",
        "  z = list(np.zeros(word_embedding, dtype=np.float32))\n",
        "  if len(sent) > seq_len:\n",
        "    return sent[:seq_len]\n",
        "  else:\n",
        "    diff = seq_len - len(sent)\n",
        "    zs = [z for each in range(diff)]\n",
        "    return zs + sent"
      ],
      "metadata": {
        "id": "rpnC4z92-Cki"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Db(data.Dataset):\n",
        "  def __init__(self, rev, labels):\n",
        "    self.rev = rev\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.rev)\n",
        "\n",
        "  def __getitem__(self, idx: int):\n",
        "    return torch.tensor(self.rev[idx]), torch.tensor(self.labels[idx])"
      ],
      "metadata": {
        "id": "F_9zUQ9luN_U"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.cli.download('en_core_web_lg')\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKBnf2FS-GwY",
        "outputId": "9f5faa31-3678-4acc-a2ab-3456ac8da4c9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.append(np.zeros((len(subj_docs)), dtype=int), np.ones((len(subj_docs)), dtype=int))\n",
        "corpus = [sent2string(d[0]).lower() for d in subj_docs] + [sent2string(d[0]).lower() for d in obj_docs]\n",
        "\n",
        "train_samples, test_samples, train_labels, test_labels = train_test_split(corpus, labels, test_size=0.3)\n",
        "\n",
        "train_samples = [create_tens(rev, seq_len, word_embedding) for rev in train_samples]\n",
        "test_samples = [create_tens(rev, seq_len, word_embedding) for rev in test_samples]\n",
        "\n",
        "# train samples are tensors of seq_len x word_embedding\n",
        "train_dataset = Db(train_samples, train_labels)\n",
        "test_dataset = Db(test_samples, test_labels)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "4yYFIJRRPEKt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thanks to lab of DL\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    \n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    \n",
        "    self.i2h = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    self.i2o = nn.Linear(hidden_size, output_size)\n",
        "  \n",
        "  def forward(self, input, hidden=None):\n",
        "    if hidden==None:\n",
        "      hidden = self.init_hidden(input.shape[0])\n",
        "    output, _ = self.i2h(input, hidden)\n",
        "    output = self.i2o(output[:, -1])\n",
        "    return output\n",
        "\n",
        "  def init_hidden(self,shape=1):\n",
        "    return torch.zeros(1, shape, self.hidden_size)"
      ],
      "metadata": {
        "id": "XkhcSBLc2c-7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "def train(rnn, optimizer, train_loader, e):\n",
        "\n",
        "  cumulative_accuracy = 0\n",
        "  samples=0\n",
        "  pbar = tqdm(train_loader)\n",
        "  for x,y in pbar:\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    outputs = rnn(x)\n",
        "    loss = criterion(outputs, y.long())\n",
        "    _, predicted = outputs.max(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    samples += x.shape[0]\n",
        "    cumulative_accuracy += predicted.eq(y).sum().item()\n",
        "    pbar.set_description('Epoch {}/{}, Train accuracy: {:.2f}'.format(e+1,epochs, cumulative_accuracy/samples*100))\n",
        "  return cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "-o1ey6Xa3fyl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(rnn, test_loader):\n",
        "\n",
        "  cumulative_accuracy = 0\n",
        "  samples=0\n",
        "  pbar = tqdm(test_loader)\n",
        "  for x,y in pbar:\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    outputs = rnn(x)\n",
        "    loss = criterion(outputs, y.long())\n",
        "    _, predicted = outputs.max(1)\n",
        "    samples += x.shape[0]\n",
        "    cumulative_accuracy += predicted.eq(y).sum().item()\n",
        "    pbar.set_description('Evaluate accuracy: {:.2f}'.format(cumulative_accuracy/samples*100))\n",
        "  return cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "ZCRFmwhhmvLC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 120\n",
        "seq_len = 50\n",
        "word_embedding = 300\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "n_categories = 2\n",
        "#accuracy 87.56\n",
        "\n",
        "rnn = RNN(word_embedding, n_hidden, n_categories)\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "evaluate(rnn, test_loader)\n",
        "\n",
        "for e in range(epochs):\n",
        "  train(rnn, optimizer, train_loader, e)\n",
        "\n",
        "evaluate(rnn, test_loader)\n"
      ],
      "metadata": {
        "id": "rzxNncVo4KV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(LSTM, self).__init__()\n",
        "    \n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.i2h = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.i2o = nn.Linear(hidden_size, output_size)\n",
        "      \n",
        "  def forward(self, input, hidden=None, cell=None):\n",
        "    if hidden==None:\n",
        "      hidden = self.init_hidden(input.shape[0])\n",
        "    if cell==None:\n",
        "      cell = self.init_hidden(input.shape[0])\n",
        "\n",
        "    output, (_,_)= self.i2h(input, (hidden,cell))\n",
        "    output = self.dropout(output)\n",
        "    output = self.i2o(output)\n",
        "    output = output[:, -1]\n",
        "    return output\n",
        "\n",
        "  def init_hidden(self,shape=1):\n",
        "    return torch.zeros(1, shape, self.hidden_size)\n",
        "    \n",
        "  def init_cell(self,shape=1):\n",
        "    return torch.zeros(1, shape, self.hidden_size)"
      ],
      "metadata": {
        "id": "06TVEXxQh8KG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTM(word_embedding, n_hidden, n_categories)\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr = 3e-4)\n",
        "\n",
        "evaluate(lstm, test_loader)\n",
        "\n",
        "for e in range(epochs):\n",
        "  train(lstm, optimizer, train_loader, e)\n",
        "  evaluate(lstm, test_loader)\n",
        "\n",
        "evaluate(lstm, test_loader)"
      ],
      "metadata": {
        "id": "T8YLWc6PlMOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a60f2e1-5248-4235-ea2d-13125e88efc9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluate accuracy: 47.07: 100%|██████████| 25/25 [00:10<00:00,  2.43it/s]\n",
            "Epoch 1/10, Train accuracy: 72.70: 100%|██████████| 59/59 [00:23<00:00,  2.49it/s]\n",
            "Evaluate accuracy: 88.20: 100%|██████████| 25/25 [00:08<00:00,  3.03it/s]\n",
            "Epoch 2/10, Train accuracy: 89.46: 100%|██████████| 59/59 [00:23<00:00,  2.50it/s]\n",
            "Evaluate accuracy: 90.83: 100%|██████████| 25/25 [00:08<00:00,  3.06it/s]\n",
            "Epoch 3/10, Train accuracy: 91.04: 100%|██████████| 59/59 [00:23<00:00,  2.50it/s]\n",
            "Evaluate accuracy: 91.57: 100%|██████████| 25/25 [00:08<00:00,  3.06it/s]\n",
            "Epoch 4/10, Train accuracy: 92.06: 100%|██████████| 59/59 [00:23<00:00,  2.50it/s]\n",
            "Evaluate accuracy: 91.60: 100%|██████████| 25/25 [00:08<00:00,  3.07it/s]\n",
            "Epoch 5/10, Train accuracy: 92.51: 100%|██████████| 59/59 [00:23<00:00,  2.50it/s]\n",
            "Evaluate accuracy: 91.10: 100%|██████████| 25/25 [00:08<00:00,  3.05it/s]\n",
            "Epoch 6/10, Train accuracy: 93.06: 100%|██████████| 59/59 [00:23<00:00,  2.49it/s]\n",
            "Evaluate accuracy: 91.87: 100%|██████████| 25/25 [00:11<00:00,  2.18it/s]\n",
            "Epoch 7/10, Train accuracy: 93.49: 100%|██████████| 59/59 [00:23<00:00,  2.49it/s]\n",
            "Evaluate accuracy: 91.73: 100%|██████████| 25/25 [00:08<00:00,  3.05it/s]\n",
            "Epoch 8/10, Train accuracy: 94.04: 100%|██████████| 59/59 [00:23<00:00,  2.49it/s]\n",
            "Evaluate accuracy: 91.90: 100%|██████████| 25/25 [00:08<00:00,  3.05it/s]\n",
            "Epoch 9/10, Train accuracy: 94.34: 100%|██████████| 59/59 [00:23<00:00,  2.49it/s]\n",
            "Evaluate accuracy: 92.17: 100%|██████████| 25/25 [00:08<00:00,  3.04it/s]\n",
            "Epoch 10/10, Train accuracy: 94.77: 100%|██████████| 59/59 [00:23<00:00,  2.49it/s]\n",
            "Evaluate accuracy: 92.27: 100%|██████████| 25/25 [00:09<00:00,  2.66it/s]\n",
            "Evaluate accuracy: 92.17: 100%|██████████| 25/25 [00:08<00:00,  3.00it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92.16666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis with RNN and LSTM**"
      ],
      "metadata": {
        "id": "CA0DeKIDf0no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "mr = movie_reviews\n",
        "neg = mr.paras(categories='neg')\n",
        "pos = mr.paras(categories='pos')"
      ],
      "metadata": {
        "id": "B0GgcF-Wj8S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus_sa_subj contains only subjectivity\n",
        "\n",
        "print(pos[0])\n",
        "print(corpus_sa[0])\n",
        "\n",
        "train_samples_sa, test_samples_sa, train_labels_sa, test_labels_sa = train_test_split(corpus_sa_subj, labels_sa_subj, test_size=0.2)\n",
        "\n",
        "train_samples_sa = [create_tens(x, 500, word_embedding) for x in train_samples_sa]\n",
        "test_samples_sa = [create_tens(x, 500, word_embedding) for x in test_samples_sa]\n",
        "\n",
        "train_dataset_sa = Db(train_samples_sa, train_labels_sa)\n",
        "test_dataset_sa = Db(test_samples_sa, test_labels_sa)\n",
        "\n",
        "train_loader_sa = torch.utils.data.DataLoader(train_dataset_sa, batch_size=batch_size, shuffle=True)\n",
        "test_loader_sa = torch.utils.data.DataLoader(test_dataset_sa, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "EWPTWCT2gEJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_sa = LSTM(word_embedding, n_hidden, n_categories)\n",
        "optimizer = torch.optim.Adam(lstm_sa.parameters(), lr = 3e-4)\n",
        "# evaluate(rnn, test_loader_sa)\n",
        "\n",
        "for e in range(epochs):\n",
        "  train(lstm_sa, optimizer_sa, train_loader_sa, e)\n",
        "  evaluate(lstm_sa, test_loader_sa) \n",
        "\n",
        "evaluate(lstm_sa, test_loader_sa)"
      ],
      "metadata": {
        "id": "aAjXCybTgYjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import subjectivity\n",
        "subjectivity.categories()\n",
        "\n",
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')]\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')]\n",
        "\n",
        "print(len(subj_docs))\n",
        "print(len(obj_docs))\n",
        "\n",
        "tot_len = 0\n",
        "for s in (subj_docs + obj_docs):\n",
        "  #print(s[0])\n",
        "  tot_len += len(s[0])\n",
        "\n",
        "print(tot_len/len(subj_docs + obj_docs))\n"
      ],
      "metadata": {
        "id": "jgkoPNvsqxur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "mr = movie_reviews\n",
        "neg = mr.paras(categories='neg')\n",
        "pos = mr.paras(categories='pos')\n",
        "\n",
        "tot_len = 0\n",
        "tkn = 0\n",
        "\n",
        "sentences = 0\n",
        "for rev in (pos + neg):\n",
        "  for sent in rev:\n",
        "    sentences += 1\n",
        "     \n",
        "    for token in sent:\n",
        "      tkn += 1\n",
        "\n",
        "print(sentences)\n",
        "print('average of sentences: {}'.format(sentences/ len(pos+neg)))\n",
        "print(tkn/len(pos + neg))"
      ],
      "metadata": {
        "id": "xXyIFMoY8h3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}